{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "“lab_wine_partial.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingY1024/introml/blob/master/lab11_yj1586.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfqV6U6ZqN7y"
      },
      "source": [
        "# Lab:  Hyper-Parameter Optimization with PCA\n",
        "\n",
        "PCA is often applied as a pre-processing step with classifiers.  When using PCA in this manner, one must select the number of PC components to use along with parameters in classifier.  In this lab, we will demonstrate how to performing this *hyper-parameter optimization*.  In doing the lab, you will learn to:\n",
        "\n",
        "* Combine PCA with data scaling.\n",
        "* Compute and visualize PC components\n",
        "* Select the number of PCs with K-fold cross validation\n",
        "* Implement the multi-stage classifier pipeline in sklearn\n",
        "* Perform automatic parameter search using `GridSearchCV` in combination with a pipeline.\n",
        "\n",
        "We first download the basic packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agjcMqOFqN7y"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzo_jLIQqN7z"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "We will use a very simple `wine` dataset, commonly used in teaching machine learning class.  The problem is to classify the type of red wine from features of the wine such as the `alchohol` and other chemical components.  There are three possible wine types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro85k27MqN7z"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import KFold\n",
        "data = load_wine()\n",
        "\n",
        "# TODO print the features names in data.feature_names and data.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZu9ZnIqN7z"
      },
      "source": [
        "Get the data matrix `X` from `data.data` and the target values `y` from `data.target`.  Print the number of samples, number of features and number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctocjt12qN7z"
      },
      "source": [
        "# TODO\n",
        "#    X = ...\n",
        "#    y = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok1FK47_qN7z"
      },
      "source": [
        "## Perform PCA for Visualization\n",
        "\n",
        "Before performing PCA, you should scale the data matrix to remove the mean and normalize the variance of the different components.  For this purpose, create a `StandardScaling` object `scaling`.  Then `fit` the scaling with the entire data `X`.  Transform the data and let `Xs` be the scaled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUFPUk1tqN7z"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO\n",
        "# scaling = ...\n",
        "# scaling.fit(...)\n",
        "# Xs = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPEfuZMbqN7z"
      },
      "source": [
        "Now, fit a PCA on the scaled data matrix `Xs`.  You can use the `sklearn` `PCA` method.  In order that we can visualize the results set `n_components=2`.  Select `svd_solver='randomized'` and `whiten=True`.  Use the the `pca.transform` method to find, `Z`, the coefficients of `Xs` in the PCA basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j3I7edsqN7z"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# TODO\n",
        "# ncomp = 2\n",
        "# pca = PCA(...)\n",
        "# Z = ...\n",
        "\n",
        "\n",
        "# Construct the PCA object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIXAhS5xqN7z"
      },
      "source": [
        "In the transformed basis, each data sample is represented by a two dimensional vector, `Z[i,0], Z[i,1]`.  Plot a scatter plot of the transformed data.  Use different marker colors for the different classes.  If you did everything, you should see that the classes are quite well separated with even two PCs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ic8YykDqN7z"
      },
      "source": [
        "# TODO\n",
        "# plt.scatter(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5mk1UJ2qN7z"
      },
      "source": [
        "Now, refit the scaled data `Xs` using `n_components=nfeatures` where `nfeatures` is the number of features.  This is the maximum number of PCs.  Get the singular values from `pca.singular_values_` and plot the portion of variation as a function of the number of PCs.   The PoV for using `n` PCs is:\n",
        "\n",
        "    PoV[n]  = \\sum_{i=0}^{n-1}  s[i]**2 / \\sum_{i=0}^{d-1}  s[i]**2\n",
        "    \n",
        "where `s[i]` is the `i`-th singular value and `d` is the number of features.  You should see that the 4 PCs contains more than 70% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMI8Ls0iqN7z"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-u6VaaZqN7z"
      },
      "source": [
        "## Using PCA with Classification \n",
        "\n",
        "We will now use data scaling and PCA as a pre-processing step for logistic classification.  The number of PCs to use can be found with cross-validation.  Complete the code below which tries different number of PCs components to use and measures the test accuracy for each value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaREglkhqN7z"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "nfold = 5\n",
        "\n",
        "# Create a K-fold object\n",
        "kf = KFold(n_splits=nfold)\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "# Number of PCs to try\n",
        "ncomp_test = np.arange(2,12)\n",
        "num_nc = len(ncomp_test)\n",
        "\n",
        "# Accuracy:  acc[icomp,ifold]  is test accuracy when using `ncomp = ncomp_test[icomp]` in fold `ifold`.\n",
        "acc = np.zeros((num_nc,nfold))\n",
        "\n",
        "# Loop over number of components to test\n",
        "for icomp, ncomp in enumerate(ncomp_test):\n",
        "    \n",
        "    # Look over the folds\n",
        "    for ifold, I in enumerate(kf.split(X)):\n",
        "        Itr, Its = I\n",
        "\n",
        "        # TODO:  Split data into training \n",
        "        # Xtr, Xts, ytr, yts = ...\n",
        "\n",
        "        # TODO:  Create a scaling object and fit the scaling on the training data\n",
        "\n",
        "        # TODO:  Fit the PCA on the scaled training data\n",
        "\n",
        "        # TODO:  Train a classifier on the transformed training data\n",
        "        # Use a logistic regression classifier\n",
        "        #   logreg = LogisticRegression(multi_class='auto', solver='lbfgs')\n",
        "\n",
        "        # TODO:  Transform the test data through data scaler and PCA\n",
        "\n",
        "        # TODO:  Predict the labels the test data\n",
        "        \n",
        "        # TODO:  Measure the accuracy \n",
        "        #    acc[icomp, ifold] = ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtuIWiKvqN7z"
      },
      "source": [
        "Use the `plt.errorbar` function to plot the mean accuracy with error bars corresponding to the standard error of the accuracy as a function of the number of components.  Find the optimal number of PCs to use according to the normal rule and one SE rule.  If you did it correctly, you should get an accuracy of around 96%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mMYp3D1qN7z"
      },
      "source": [
        "# TODO:\n",
        "# acc_mean = ...\n",
        "# acc_se = ...\n",
        "# plt.errorbar(...)\n",
        "\n",
        "# TODO:  Optimal order with the normal rule\n",
        "\n",
        "# TODO:  Optimal order with one SE rule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH33XSoBqN7z"
      },
      "source": [
        "## Hyper-Parameter Optimization with GridCV.\n",
        "\n",
        "We will now try a more complex classifier -- a support vector classifier with a radial basis function.  When we use such a classifer, there will be a number of parameters to tune.  When the number of parameters to tune becomes large, writing a loop over multiple parameters as we did above becomes cumbersome.  The `sklearn` package has a very nice routine, `GridSearchCV` to perform this sort of parameter search.  \n",
        "\n",
        "Before, we do this we need to create an estimator `Pipeline`.  An estimator pipeline is a sequence of transformations followed by an estimator that will operate on the transformed data.  Create the following pipeline:\n",
        "\n",
        "*  Create a `StandardScaler()` object called `scaler` for the first transformation\n",
        "*  Create a `PCA()` object called `pca` for the second transformation\n",
        "*  Create a `SVC()` object called `svc` for the final SVM classifier.  Set the parameter `kernel='rbf'`.\n",
        "\n",
        "Once you have the three steps defined, you can create the pipeline with the command:\n",
        "    \n",
        "    pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('svc', svc)])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8gLjw5qN7z"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# TODO\n",
        "# scaler = ...\n",
        "# pca = ...\n",
        "# svc = ...\n",
        "# pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('svc', svc)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhdbaHHOqN7z"
      },
      "source": [
        "We next define all the parameters that we want to search over.  Define the following arrays:\n",
        "\n",
        "*  `ncomp_test`:  values from 3 to 10 representing number of PCs to test\n",
        "*  `C_test`:  values of `C` in the SVC to test.  Use `10^{-2}, 10^{-1}, ... ,10^{3}` \n",
        "*  `gam_test`:  values of `gamma` in the SVC to test.  Use `10^{-3}, 10^{-2}, ... ,10^{1}` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bisJOOJmqN7z"
      },
      "source": [
        "# TODO\n",
        "# ncomp_test = ...\n",
        "# c_test = ...\n",
        "# gam_test = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMtAibbRqN7z"
      },
      "source": [
        "Next, we create a dictionary `params` of the form:\n",
        "\n",
        "    params =  {'pca__n_components': ncomp_test, 'svc__C' : c_test, ...}\n",
        "    \n",
        "Each key in the dictionary is the of the form `estimator__param` and the value is the values to be tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iClUjCm-qN7z"
      },
      "source": [
        "# TODO\n",
        "# params = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ-lMngdqN7z"
      },
      "source": [
        "Finally, an object `estimator = GridSearchCV(...)` from  `pipe` and `params`.  Set `cv=5`, `train_score=True` and `iid=False`.  Fit the estimator from the data `X,y`.  Then the estimator will perform the cross-validation over all the parameters.  This may take a minute since we are search over so many parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvrrzBtqqN7z"
      },
      "source": [
        "# TODO\n",
        "# estimator = GridSearchCV(...)\n",
        "# estimator.fit(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gZ3lSC-qN7z"
      },
      "source": [
        "Print the best test score and best parameters.  They are fields in `estimator`.  If you did it correctly, it should be a little higher than the logistic regression (about 0.97 to 0.98 accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2dfb1RGqN7z"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WlC5CvKqN7z"
      },
      "source": [
        "Finally, you can get the test score for all the parameter choices from \n",
        "\n",
        "    test_score = estimator.cv_results_['mean_test_score']\n",
        "    \n",
        "Use the `imshow` command to plot the mean test score over `gamma` and `C` for the value `n_components=5`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrzErP7nqN7z"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4HDU1sxqN7z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_EjZPlqN7z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}